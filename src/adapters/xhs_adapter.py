"""
é€‚é…å±‚ (Adapter Layer) - å°çº¢ä¹¦é€‚é…å™¨

èŒè´£:
1. å°çº¢ä¹¦å¹³å°ç‰¹å®šçš„ HTML å…ƒç´ å®šä½ï¼ˆè‡ªæ„ˆå¼æå–ï¼‰
2. æ•°æ®æå–å’Œç»“æ„åŒ–ï¼ˆç¬”è®°ã€ä½œè€…ã€äº’åŠ¨æ•°æ®ï¼‰
3. å¤„ç†å°çº¢ä¹¦çš„åçˆ¬æœºåˆ¶ï¼ˆå¦‚åŠ¨æ€åŠ è½½ã€æ‡’åŠ è½½ï¼‰
4. API å—…æ¢ï¼ˆNetwork Sniffingï¼‰
5. ä¸‰å±‚é™çº§æŠ“å–ä½“ç³»ï¼ˆAPI â†’ HTML â†’ Mockï¼‰

æ ¸å¿ƒæŠ€æœ¯è§„åˆ™ï¼ˆé‡‘ç§‘ç‰å¾‹ï¼‰ï¼š
- ç¬¬ä¸€å±‚ï¼ˆä¼˜å…ˆï¼‰ï¼šNetwork Sniffing æˆªè·åŸå§‹ JSON
- ç¬¬äºŒå±‚ï¼ˆè‡ªæ„ˆï¼‰ï¼šå¯å‘å¼ XPath åŸºäºè§†è§‰ç‰¹å¾
- ç¬¬ä¸‰å±‚ï¼ˆä¿åº•ï¼‰ï¼šæ™ºèƒ½ Mock ç¡®ä¿æµç¨‹ä¸ä¸­æ–­
"""

from typing import List, Dict, Any, Optional
from datetime import datetime
import asyncio
import os
import re
from urllib.parse import quote
from loguru import logger
from src.core.base_spider import BaseSpider
from src.core.extraction_engine import ExtractionEngine
from config import (
    NetworkInterceptorConfig,
)


class XiaohongshuAdapter(BaseSpider):
    """å°çº¢ä¹¦é€‚é…å™¨ - ç»§æ‰¿åŸºç¡€çˆ¬è™«èƒ½åŠ› + ä¸‰å±‚é™çº§æŠ“å–"""
    
    def __init__(
        self,
        config_path: str = "config.yaml",
        debug_mode: bool = False,
        use_persistent_session: bool = True,
        use_api_sniffing: bool = True,
        use_context_pool: Optional[bool] = None,
    ):
        super().__init__(
            config_path,
            debug_mode,
            platform='xiaohongshu',
            use_persistent_session=use_persistent_session,
            use_context_pool=use_context_pool,
        )
        self.xhs_config = self.config.xiaohongshu
        self.base_url = self.xhs_config.base_url
        self.use_api_sniffing = use_api_sniffing
        
        
        # åˆå§‹åŒ–ä¸‰å±‚é™çº§æŠ“å–å¼•æ“
        self.extraction_engine = ExtractionEngine()
        
        logger.success("ğŸ­ å°çº¢ä¹¦é€‚é…å™¨å·²å¯ç”¨: ä¸‰å±‚é™çº§æŠ“å– + Stealth 2.0")
    
    async def search_notes(
        self,
        keyword: str,
        max_pages: int = None,
        fetch_detail: bool = False,
        detail_limit: int = 5,
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢å°çº¢ä¹¦ç¬”è®°ï¼ˆä¸‰å±‚é™çº§æŠ“å–ä½“ç³»ï¼‰
        
        æ‰§è¡Œé¡ºåº:
        1. Network Sniffing (page.on('response')) - ä¼˜å…ˆ
        2. å¯å‘å¼ XPath (è§†è§‰ç‰¹å¾) - è‡ªæ„ˆ
        3. æ™ºèƒ½ Mock (æ¨¡æ‹Ÿæ•°æ®) - ä¿åº•
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_pages: æœ€å¤§çˆ¬å–é¡µæ•°
            
        Returns:
            ç¬”è®°åˆ—è¡¨ï¼ˆå¸¦ _extraction_source æ ‡è®°ï¼‰
        """
        max_pages = max_pages or self.xhs_config.max_pages
        
        logger.info(f"ğŸ” å¼€å§‹æœç´¢å°çº¢ä¹¦å…³é”®è¯: {keyword} (ä¸‰å±‚é™çº§æ¨¡å¼)")

        # é¢„çƒ­ explore å¹¶ç¡®ä¿å·²ç™»å½•ï¼Œé¿å…æœç´¢é¡µé‡å®šå‘åˆ°ç™»å½•å¯¼è‡´ç©ºç™½
        await self.ensure_login_ready()
        
        # å…ˆæŒ‚è½½ç½‘ç»œæ‹¦æˆªå™¨ï¼Œå†è®¿é—®æœç´¢é¡µï¼Œç¡®ä¿é¦–å±è¯·æ±‚è¢«æ•è·
        if self.use_api_sniffing:
            await self.setup_network_interceptor(NetworkInterceptorConfig.XIAOHONGSHU_APIS)

        # è®¿é—®æœç´¢é¡µ
        # å¼ºåˆ¶è½åœ¨ã€Œç¬”è®°ã€tabï¼ˆtype=51ï¼‰ï¼Œä»¥ç¨³å®šè§¦å‘ /api/sns/web/v1/search/notes
        # è¯´æ˜ï¼šä¸å¼•å…¥ &source=web_explore_feedï¼ˆæŒ‰ä½ çš„è¦æ±‚ç§»é™¤ï¼‰ï¼Œä»…è¡¥è¶³å¿…è¦çš„ typeã€‚
        kw = quote(str(keyword), safe="")
        search_url = f"{self.base_url}/search_result?keyword={kw}&type=51"
        await self.goto(search_url)
        
        # æ™ºèƒ½ç­‰å¾…å†…å®¹åŠ è½½
        await self.wait_for_load_state('networkidle', timeout=10000)

        # æœç´¢é¡µæ¸²æŸ“è‡ªæ„ˆï¼šè‹¥æœªè§åˆ°å¡ç‰‡/é”šç‚¹ï¼Œå°è¯• reload + è½»æ»šåŠ¨
        await self._ensure_search_page_rendered()

        # è‹¥å‡ºç°ç™»å½•é®ç½©/å¼¹çª—ï¼Œå°½é‡å…³é—­ï¼Œé¿å…é®æŒ¡æ¸²æŸ“/æ»šåŠ¨
        await self._dismiss_login_prompts()

        # å¦‚æœä»æ£€æµ‹åˆ°ç™»å½•æç¤ºï¼Œå°è¯•å…ˆè®¿é—® explore å†å›åˆ°æœç´¢ï¼Œè§¦å‘ä¼šè¯åŠ è½½
        await self._rehit_search_if_login_prompt(keyword, search_url)

        # å°½é‡åˆ‡åˆ°ã€Œç¬”è®°ã€ç»“æœé¡µç­¾ï¼Œé¿å…è½åœ¨ç»¼åˆ/å…¶ä»–ç±»å‹å¯¼è‡´å¡ç‰‡/æ¥å£åå°‘
        try:
            await self._try_switch_to_notes_tab()
        except Exception:
            pass

        # è‹¥é¦–å±ä»æ— å¡ç‰‡ï¼Œç›´æ¥åœ¨æœç´¢æ¡†é‡æ–°æäº¤å…³é”®è¯ä»¥è§¦å‘è¯·æ±‚
        try:
            await self._nudge_search_if_empty(keyword)
        except Exception:
            pass

        # è½»é‡ç”¨æˆ·è¡Œä¸ºè§¦å‘ï¼šæ»šåŠ¨ä¸€ä¸‹ï¼Œè®©é¡µé¢æŒ‰â€œæ­£å¸¸ç”¨æˆ·â€è·¯å¾„å‘èµ·è¯·æ±‚
        try:
            await self.human_scroll()
            await asyncio.sleep(1.5)
        except Exception:
            pass

        # æ³¨æ„ï¼šæ­¤å‰çš„ä¸»åŠ¨ API è§¦å‘ä¼šåœ¨éƒ¨åˆ†è´¦å·ä¸Šè¿”å›â€œè´¦å·å¼‚å¸¸â€ï¼Œé»˜è®¤ç¦ç”¨ã€‚
        # å¦‚éœ€å¼ºåˆ¶è§¦å‘ç”¨äºè°ƒè¯•ï¼Œå¯è®¾ç½®ç¯å¢ƒå˜é‡ XHS_FORCE_TRIGGER_API=1
        if self.use_api_sniffing and str(os.getenv('XHS_FORCE_TRIGGER_API', '')).lower() in {'1', 'true', 'yes'}:
            await self._trigger_search_api(keyword)
        
        # å®šä¹‰ä¸‰å±‚æå–å™¨
        async def api_extractor():
            """ç¬¬ä¸€å±‚ï¼šNetwork Sniffing"""
            if self.use_api_sniffing:
                logger.info("ğŸ§ [Layer 1] å¯åŠ¨ Network Sniffing...")
                notes = await self._extract_from_api()
                if notes:
                    return notes

                # å…œåº•ç­–ç•¥ï¼šæœªæ‹¦æˆªåˆ° search åŒ…æ—¶ï¼Œå†åˆ‡æ¢ã€Œç¬”è®°ã€+æ»šåŠ¨+ä¸»åŠ¨è°ƒæ¥å£
                logger.warning("âš ï¸ æœªæ‹¦æˆªåˆ° search æ•°æ®ï¼Œå°è¯•å…œåº•ï¼šåˆ‡æ¢ç¬”è®°é¡µç­¾+æ»šåŠ¨+ä¸»åŠ¨æœç´¢ API")
                try:
                    await self._try_switch_to_notes_tab()
                    await self.human_scroll()
                    await asyncio.sleep(1.2)
                except Exception:
                    pass

                try:
                    await self._trigger_search_api(keyword)
                    await asyncio.sleep(1.0)
                except Exception:
                    logger.debug("ä¸»åŠ¨æœç´¢ API å…œåº•å¤±è´¥(å·²å¿½ç•¥)")

                return await self._extract_from_api()
            return []
        
        async def html_extractor():
            logger.info("ğŸ” [Layer 2] å¯ç”¨æœ€å° DOM æå– (Fallback)")
            return await self._extract_from_dom_cards(limit=max_pages * 20)
        
        # æ‰§è¡Œä¸‰å±‚é™çº§æŠ“å–
        all_notes = await self.extraction_engine.extract_with_fallback(
            api_extractor=api_extractor,
            html_extractor=html_extractor,
            mock_generator=None,  # å…³é—­ Mockï¼Œç¡®ä¿åªè¿”å›çœŸå®æ•°æ®
            context={
                'keyword': keyword,
                'count': max_pages * 20,
                'platform': 'xiaohongshu'
            }
        )
        
        # æ·»åŠ é€šç”¨å­—æ®µ
        for note in all_notes:
            note['platform'] = 'xiaohongshu'
            note['keyword'] = keyword
            note['crawl_time'] = datetime.now().isoformat()

        # è§¦å‘è¯¦æƒ…é¡µè¯·æ±‚ä»¥æ‹¦æˆª note_detailï¼ˆå¯é€‰ï¼‰
        if fetch_detail and self.use_api_sniffing:
            await self._warm_note_details(all_notes, limit=detail_limit)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        stats = self.extraction_engine.get_stats()
        logger.info(f"ğŸ“Š æå–ç»Ÿè®¡: {stats}")
        logger.success(f"âœ… æœç´¢å®Œæˆï¼å…±æå– {len(all_notes)} æ¡ç¬”è®°")
        
        return all_notes[:max_pages * 20]  # é™åˆ¶æ•°é‡

    async def _extract_from_dom_cards(self, limit: int = 60) -> List[Dict[str, Any]]:
        """æœ€å° DOM æå–ï¼šä»æœç´¢ç»“æœé¡µä¸­æŠ“å–å¡ç‰‡é“¾æ¥ä¸æ–‡æœ¬ã€‚

        ç›®æ ‡ï¼šåœ¨ API å—…æ¢è¢«é£æ§/åŠ å¯†æ—¶ï¼Œä»èƒ½è¿”å›å¯ç”¨çš„ note_id/url/titleã€‚
        """
        if not self.page:
            return []

        try:
            items = await self.page.evaluate(
                                r"""
                (limit) => {
                  const uniq = new Set();
                  const out = [];
                  const anchors = Array.from(document.querySelectorAll('a[href]'))
                    .filter(a => (a.getAttribute('href') || '').includes('/explore/'));

                  for (const a of anchors) {
                    if (out.length >= limit) break;
                    const href = a.getAttribute('href') || '';
                    const m = href.match(/\/explore\/(\w+)/);
                    const noteId = m ? m[1] : null;
                    const url = href.startsWith('http') ? href : (location.origin + href);
                    const text = (a.innerText || '').trim();

                    if (!noteId || uniq.has(noteId)) continue;
                    uniq.add(noteId);

                    out.push({
                      note_id: noteId,
                      url,
                      title: text ? text.split(/\n|\r/)[0].slice(0, 80) : '',
                      raw_text: text.slice(0, 200),
                    });
                  }
                  return out;
                }
                """,
                limit,
            )
        except Exception:
            items = []

        results: List[Dict[str, Any]] = []
        for it in items or []:
            if not isinstance(it, dict):
                continue
            note_id = it.get('note_id')
            if not note_id:
                continue
            results.append({
                'note_id': note_id,
                'url': it.get('url'),
                'title': it.get('title') or it.get('raw_text') or '',
                'source': 'html',
            })
        return results

    async def _ensure_search_page_rendered(self) -> None:
        if not self.page:
            return

        selectors = [
            ".note-item",
            "[data-note-id]",
            "a[href*='/explore/']",
        ]

        for attempt in range(2):
            for sel in selectors:
                try:
                    await self.page.wait_for_selector(sel, timeout=3500)
                    logger.debug(f"ğŸ–¼ï¸ æœç´¢é¡µæ¸²æŸ“æ£€æµ‹é€šè¿‡: {sel}")
                    return
                except Exception:
                    continue

            # æœªæ£€æµ‹åˆ°å¡ç‰‡ï¼Œå°è¯•è‡ªæ„ˆï¼šreload + è½»æ»šåŠ¨
            try:
                logger.info("ğŸ”„ æœç´¢é¡µç–‘ä¼¼æœªå®Œå…¨æ¸²æŸ“ï¼Œå°è¯• reload + è½»æ»šåŠ¨ è‡ªæ„ˆâ€¦")
                await self.page.reload(wait_until='domcontentloaded', timeout=15000)
                await self.wait_for_load_state('networkidle', timeout=12000)
                try:
                    await self.human_scroll()
                    await asyncio.sleep(0.8)
                except Exception:
                    pass
            except Exception as exc:
                logger.debug(f"reload è‡ªæ„ˆå¤±è´¥(å·²å¿½ç•¥): {exc}")

        logger.warning("âš ï¸ æœç´¢é¡µæœªæ£€æµ‹åˆ°å¡ç‰‡å…ƒç´ ï¼Œå¯èƒ½æ¸²æŸ“ä¸å…¨æˆ–è¢«é£æ§")

    async def _dismiss_login_prompts(self) -> None:
        if not self.page:
            return
        selectors = [
            "button:has-text('ç™»å½•')",
            "button:has-text('å»ç™»å½•')",
            "button:has-text('å–æ¶ˆ')",
            "[aria-label='å…³é—­']",
            "svg[aria-label='å…³é—­']",
            "div[class*='modal'] button",
        ]
        for sel in selectors:
            try:
                loc = self.page.locator(sel).first
                if await loc.count() == 0:
                    continue
                await loc.click(timeout=1000)
                await asyncio.sleep(0.5)
                logger.debug(f"ğŸ§¹ å·²å°è¯•å…³é—­ç™»å½•æç¤º: {sel}")
                return
            except Exception:
                continue

    async def _rehit_search_if_login_prompt(self, keyword: str, search_url: str) -> None:
        if not self.page:
            return
        try:
            body = (await self.page.content() or "").lower()
        except Exception:
            body = ""

        login_signals = ["ç™»å½•", "login", "account-login", "verify"]
        if any(sig in body for sig in login_signals):
            try:
                logger.info("ğŸ” æœç´¢é¡µä»æœ‰ç™»å½•æç¤ºï¼Œå…ˆè®¿é—® explore å†è¿”å›æœç´¢ä»¥åŠ è½½ä¼šè¯")
                await self.goto(f"{self.base_url}/explore")
                await self.wait_for_load_state('networkidle', timeout=10000)
                try:
                    await self.human_scroll()
                    await asyncio.sleep(0.8)
                except Exception:
                    pass
                await self.goto(search_url)
                await self.wait_for_load_state('networkidle', timeout=10000)
            except Exception as exc:
                logger.debug(f"rehit search after explore å¤±è´¥(å·²å¿½ç•¥): {exc}")

    async def _try_switch_to_notes_tab(self) -> None:
        if not self.page:
            return
        candidates = [
            "[role='tab']:has-text('ç¬”è®°')",
            "a:has-text('ç¬”è®°')",
            "div:has-text('ç¬”è®°')",
            "span:has-text('ç¬”è®°')",
            # æ–°ç‰ˆå¯¼èˆªæ–‡æ¡ˆï¼ˆâ€œç¬”è®°â€æ”¹ä¸ºâ€œå›¾æ–‡â€æˆ–â€œå…¨éƒ¨â€ï¼‰
            "[role='tab']:has-text('å›¾æ–‡')",
            "a:has-text('å›¾æ–‡')",
            "div:has-text('å›¾æ–‡')",
            "span:has-text('å›¾æ–‡')",
            "[role='tab']:has-text('å…¨éƒ¨')",
            "a:has-text('å…¨éƒ¨')",
            "div:has-text('å…¨éƒ¨')",
            "span:has-text('å…¨éƒ¨')",
        ]
        for selector in candidates:
            try:
                loc = self.page.locator(selector).first
                if await loc.count() == 0:
                    continue
                await loc.click(timeout=1500)
                await asyncio.sleep(0.8)
                return
            except Exception:
                continue

    async def _retry_search_via_input(self, keyword: str) -> None:
        """å½“é¦–å±æ— å¡ç‰‡æ—¶ï¼Œç›´æ¥åœ¨æœç´¢æ¡†å†æ¬¡æäº¤å…³é”®è¯ã€‚"""
        if not self.page:
            return

        input_selectors = [
            "input[placeholder*='æœç´¢']",
            "input[type='search']",
            "input[type='text']",
        ]
        search_btn_candidates = [
            "button:has-text('æœç´¢')",
            "button[aria-label*='æœç´¢']",
            "svg[aria-label*='æœç´¢']",
            "[data-testid*='search']",
        ]

        for sel in input_selectors:
            try:
                loc = self.page.locator(sel).first
                if await loc.count() == 0:
                    continue
                await loc.click(timeout=1500)
                try:
                    await loc.fill(keyword, timeout=1500)
                except Exception:
                    # å›é€€ï¼šå…¨é€‰+é”®ç›˜è¾“å…¥
                    await loc.press("Control+A")
                    await loc.type(keyword, delay=30)

                # ä¼˜å…ˆç‚¹å‡»æœç´¢æŒ‰é’®ï¼Œå…¶æ¬¡å›è½¦
                clicked = False
                for btn_sel in search_btn_candidates:
                    btn = self.page.locator(btn_sel).first
                    if await btn.count() == 0:
                        continue
                    try:
                        await btn.click(timeout=1200)
                        clicked = True
                        break
                    except Exception:
                        continue

                if not clicked:
                    await loc.press("Enter")

                await self.wait_for_load_state('networkidle', timeout=10000)
                try:
                    await self.human_scroll()
                    await asyncio.sleep(0.8)
                except Exception:
                    pass
                return
            except Exception:
                continue

    async def _nudge_search_if_empty(self, keyword: str) -> None:
        """é¦–å±æœªå‡ºç°å¡ç‰‡æ—¶ï¼Œä¸»åŠ¨é‡æ–°æäº¤å…³é”®è¯ä¸€æ¬¡ã€‚"""
        if not self.page:
            return

        try:
            count = await self.page.evaluate(
                """
                () => document.querySelectorAll('a[href*="/explore/"]').length
                """
            )
        except Exception:
            count = 0

        if count and count > 0:
            return

        logger.warning("âš ï¸ æœç´¢é¡µæœªæ£€æµ‹åˆ°å¡ç‰‡ï¼Œå°è¯•é‡æ–°æäº¤å…³é”®è¯ä»¥è§¦å‘åŠ è½½â€¦")
        await self._retry_search_via_input(keyword)

    async def _extract_from_api(self) -> List[Dict[str, Any]]:
        """
        ä»æ‹¦æˆªçš„ API æ•°æ®ä¸­æå–ç¬”è®°ï¼ˆNetwork Sniffingï¼‰
        
        Returns:
            ç¬”è®°åˆ—è¡¨
        """
        import asyncio
        
        # ç­‰å¾… API å“åº”
        await asyncio.sleep(2)
        
        # è·å–æ‹¦æˆªçš„ API æ•°æ®ï¼ˆsearch + feed + note_detailï¼‰
        api_responses = []
        for api_name in ("search", "feed", "note_detail"):
            api_responses.extend([
                {
                    'name': api_name,
                    'data': resp
                }
                for resp in self.get_api_responses(api_name)
            ])

        if not api_responses:
            logger.info("âš ï¸ æœªæ‹¦æˆªåˆ° API æ•°æ®ï¼Œä½¿ç”¨ HTML æå–")
            return []

        all_notes = []

        for response in api_responses:
            raw = response.get('data')
            payload = raw.get('data') if isinstance(raw, dict) else raw

            # æå–å¹¶æ˜ å°„æ•°æ®
            notes = self.extract_from_api(
                api_data=payload,
                data_path=NetworkInterceptorConfig.XIAOHONGSHU_APIS[response['name']]['data_path'],
                mapping=NetworkInterceptorConfig.XIAOHONGSHU_MAPPING
            )
            
            # æ·»åŠ é¢å¤–å­—æ®µ
            for note in notes:
                note['platform'] = 'xiaohongshu'
                note['crawl_time'] = datetime.now().isoformat()
                note['source'] = 'api'  # æ ‡è®°æ•°æ®æ¥æº

                # æ„é€ å¯è®¿é—®çš„è¯¦æƒ…é“¾æ¥ï¼ˆéœ€è¦ xsec_tokenï¼‰
                nid = note.get('note_id')
                token = note.get('xsec_token')
                if nid and token and not note.get('url'):
                    note['url'] = f"{self.base_url}/explore/{nid}?xsec_token={token}&xsec_source=pc_search"
            
            all_notes.extend(notes)

        if not all_notes:
            logger.warning("âš ï¸ [Layer 1] API æå–ç»“æœä¸ºç©º")
        else:
            logger.success(f"âœ… [Layer 1] API æå–æˆåŠŸ: {len(all_notes)} æ¡")
        
        return all_notes

    async def _warm_note_details(self, notes: List[Dict[str, Any]], limit: int = 5) -> None:
        """ä¾æ¬¡è®¿é—®ç¬”è®°è¯¦æƒ…é¡µï¼Œè§¦å‘ note_detail æ¥å£ä»¥ä¾¿æ‹¦æˆªã€‚

        Args:
            notes: å·²æå–çš„ç¬”è®°åˆ—è¡¨ï¼ˆéœ€åŒ…å« note_idï¼‰
            limit: æœ€å¤šè§¦å‘çš„ç¬”è®°æ•°é‡ï¼Œé¿å…è¿‡å¤šè·³è½¬
        """
        if not notes or limit <= 0:
            return

        # å»é‡å¹¶æˆªæ–­
        note_ids = []
        seen = set()
        for item in notes:
            nid = item.get('note_id')
            if nid and nid not in seen:
                seen.add(nid)
                note_ids.append(nid)
            if len(note_ids) >= limit:
                break

        if not note_ids:
            return

        logger.info(f"ğŸ¯ è§¦å‘ {len(note_ids)} æ¡ç¬”è®°è¯¦æƒ…ä»¥æ‹¦æˆª note_detail API")

        for idx, nid in enumerate(note_ids, 1):
            try:
                token = None
                for item in notes:
                    if item.get('note_id') == nid and item.get('xsec_token'):
                        token = item.get('xsec_token')
                        break

                if token:
                    detail_url = f"{self.base_url}/explore/{nid}?xsec_token={token}&xsec_source=pc_search"
                else:
                    detail_url = f"{self.base_url}/explore/{nid}"
                await self.goto(detail_url)
                await self.wait_for_load_state('networkidle', timeout=8000)
                await asyncio.sleep(1.5)  # ç•™æ—¶é—´è®© note_detail è¿”å›
                logger.debug(f"âœ… è¯¦æƒ…è§¦å‘ {idx}/{len(note_ids)}: {nid}")
            except Exception as exc:
                logger.warning(f"âš ï¸ è¯¦æƒ…è§¦å‘å¤±è´¥ {nid}: {exc}")

    async def _trigger_search_api(self, keyword: str) -> None:
        """ä¸»åŠ¨è¯·æ±‚æœç´¢ APIï¼Œå¸®åŠ©åœ¨é¦–å±å°±æ•è· JSONã€‚

        ä¾èµ–å·²æœ‰ç™»å½• Cookieï¼Œè‹¥æœªç™»å½•å¯èƒ½è¿”å› 401 æˆ–ç©ºæ•°æ®ã€‚
        """
        try:
            from urllib.parse import quote

            kw = quote(keyword, safe="")
            candidates = [
                (
                    "GET",
                    "https://edith.xiaohongshu.com/api/sns/web/v1/search/notes"
                    f"?keyword={kw}&page=1&page_size=20&sort=general&note_type=0"
                    "&image_formats=jpg,webp,avif",
                    None,
                ),
                (
                    "GET",
                    f"{self.base_url}/api/sns/web/v1/search/notes"
                    f"?keyword={kw}&page=1&page_size=20&sort=general&note_type=0"
                    "&image_formats=jpg,webp,avif",
                    None,
                ),
                (
                    "POST",
                    "https://edith.xiaohongshu.com/api/sns/web/v1/search/notes",
                    {
                        "keyword": keyword,
                        "page": 1,
                        "page_size": 20,
                        "sort": "general",
                        "note_type": 0,
                        "image_formats": "jpg,webp,avif",
                    },
                ),
            ]

            for method, url, payload in candidates:
                try:
                    if method == "POST":
                        resp = await self.page.request.post(url, data=payload)
                    else:
                        resp = await self.page.request.get(url)
                    status = resp.status
                    if status != 200:
                        try:
                            text = await resp.text()
                            text = (text or "").strip().replace("\n", " ")
                            text = text[:200]
                        except Exception:
                            text = ""
                        logger.debug(f"ğŸ”„ ä¸»åŠ¨è§¦å‘æœç´¢ API({method}): {url} -> {status} {text}")
                    else:
                        logger.debug(f"ğŸ”„ ä¸»åŠ¨è§¦å‘æœç´¢ API({method}): {url} -> {status}")

                        # æ³¨æ„ï¼špage.request ä¸ä¼šè§¦å‘ page.on('response')ï¼Œå› æ­¤éœ€è¦æ‰‹åŠ¨çŒå…¥æ‹¦æˆªç¼“å­˜
                        # ä»¥ä¾¿åç»­ Network Sniffing å±‚å¯ç›´æ¥æ¶ˆè´¹ã€‚
                        if method == "POST":
                            try:
                                json_data = await resp.json()

                                # Debugï¼šè½ç›˜ä¸€ä»½ API å“åº”æ ·æœ¬ï¼Œä¾¿äºæ ¡å‡† data_pathï¼ˆä»… debug_modeï¼‰
                                try:
                                    if getattr(self, 'debug_mode', False):
                                        from pathlib import Path
                                        import json as _json
                                        from datetime import datetime as _dt

                                        Path('logs').mkdir(exist_ok=True)
                                        ts = _dt.now().strftime('%Y%m%d_%H%M%S')
                                        dump_path = Path('logs') / f"debug_xhs_search_api_{ts}.json"
                                        dump_path.write_text(_json.dumps(json_data, ensure_ascii=False, indent=2), encoding='utf-8')
                                        logger.info(f"ğŸ§¾ å·²å†™å…¥æœç´¢ API æ ·æœ¬: {dump_path}")
                                except Exception:
                                    pass

                                if not hasattr(self, 'intercepted_apis'):
                                    self.intercepted_apis = {}
                                self.intercepted_apis.setdefault('search', []).append({
                                    'url': url,
                                    'method': method,
                                    'status': status,
                                    'data': json_data,
                                    'timestamp': datetime.now().isoformat(),
                                })
                                logger.success("âœ… å·²å°†ä¸»åŠ¨æœç´¢ API å“åº”å†™å…¥æ‹¦æˆªç¼“å­˜ (search)")
                            except Exception as inject_exc:
                                logger.debug(f"âš ï¸ å†™å…¥æ‹¦æˆªç¼“å­˜å¤±è´¥: {inject_exc}")
                        break
                except Exception as inner_exc:
                    logger.debug(f"ğŸ”„ ä¸»åŠ¨è§¦å‘æœç´¢ API({method}) å¤±è´¥: {url} -> {inner_exc}")
        except Exception as exc:
            logger.warning(f"âš ï¸ ä¸»åŠ¨è§¦å‘æœç´¢ API å¤±è´¥: {exc}")
    
    # è‡ªæ„ˆå¼åŠŸèƒ½å·²ç§»é™¤ï¼Œå·²ä¼˜åŒ–ä¸ºä¸‰å±‚é™çº§æå–ï¼ˆAPI â†’ HTML â†’ Mockï¼‰
    
    async def _load_more_notes(self) -> None:
        """
        åŠ è½½æ›´å¤šç¬”è®°ï¼ˆå°çº¢ä¹¦é‡‡ç”¨æ— é™æ»šåŠ¨ï¼‰
        """
        # æ»šåŠ¨åˆ°åº•éƒ¨è§¦å‘åŠ è½½
        await self.scroll_to_bottom(max_scrolls=3, delay_range=(1, 2))
        
        # æ™ºèƒ½ç­‰å¾…æ–°å†…å®¹åŠ è½½ï¼ˆç­‰å¾…ç½‘ç»œç©ºé—²ï¼‰
        await self.wait_for_load_state('networkidle', timeout=5000)
        
        logger.debug("å·²è§¦å‘åŠ è½½æ›´å¤šå†…å®¹")
    
    async def get_note_detail(self, note_url: str) -> Dict[str, Any]:
        """
        è·å–ç¬”è®°è¯¦æƒ…
        
        Args:
            note_url: ç¬”è®°é“¾æ¥
            
        Returns:
            è¯¦ç»†æ•°æ®
        """
        logger.info(f"æ­£åœ¨è·å–ç¬”è®°è¯¦æƒ…: {note_url}")
        
        await self.goto(note_url)
        
        # æ™ºèƒ½ç­‰å¾…ç¬”è®°å†…å®¹åŠ è½½
        await self.wait_for_load_state('networkidle', timeout=10000)
        await self.wait_for_selector('.note-content, .content', timeout=10000)
        
        # æå–è¯¦æƒ…æ•°æ®
        detail = {}
        
        try:
            # æ ‡é¢˜
            title_elem = await self.page.query_selector('.title, h1')
            detail['title'] = await title_elem.inner_text() if title_elem else ""
            
            # å†…å®¹
            content_elem = await self.page.query_selector('.note-content, .desc')
            detail['content'] = await content_elem.inner_text() if content_elem else ""
            
            # ä½œè€…ä¿¡æ¯
            author_elem = await self.page.query_selector('.author-name, .user-name')
            detail['author'] = await author_elem.inner_text() if author_elem else ""
            
            # äº’åŠ¨æ•°æ®
            likes_elem = await self.page.query_selector('[class*="like-count"]')
            detail['likes'] = await self._extract_number(likes_elem) if likes_elem else 0
            
            comments_elem = await self.page.query_selector('[class*="comment-count"]')
            detail['comments'] = await self._extract_number(comments_elem) if comments_elem else 0
            
            collects_elem = await self.page.query_selector('[class*="collect-count"]')
            detail['collects'] = await self._extract_number(collects_elem) if collects_elem else 0
            
            # å‘å¸ƒæ—¶é—´
            time_elem = await self.page.query_selector('.publish-time, .date')
            detail['publish_time'] = await time_elem.inner_text() if time_elem else ""
            
            # æ ‡ç­¾
            tag_elems = await self.page.query_selector_all('.tag, .hashtag')
            detail['tags'] = [await tag.inner_text() for tag in tag_elems]
            
            detail['url'] = note_url
            detail['crawl_time'] = datetime.now().isoformat()
            
            logger.success(f"è¯¦æƒ…è·å–æˆåŠŸ: {detail['title'][:30]}...")
            
        except Exception as e:
            logger.error(f"è·å–ç¬”è®°è¯¦æƒ…å¤±è´¥: {e}")
            await self.screenshot(f"./logs/xhs_detail_error_{datetime.now().strftime('%H%M%S')}.png")
        
        return detail
    
    async def batch_search(self, keywords: List[str], max_pages: int = 3) -> Dict[str, List[Dict]]:
        """
        æ‰¹é‡æœç´¢å¤šä¸ªå…³é”®è¯
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            max_pages: æ¯ä¸ªå…³é”®è¯çš„æœ€å¤§é¡µæ•°
            
        Returns:
            {å…³é”®è¯: [ç¬”è®°åˆ—è¡¨]}
        """
        results = {}
        
        for idx, keyword in enumerate(keywords, 1):
            logger.info(f"[{idx}/{len(keywords)}] å¼€å§‹æœç´¢å…³é”®è¯: {keyword}")
            
            try:
                notes = await self.search_notes(keyword, max_pages)
                results[keyword] = notes
                logger.success(f"å…³é”®è¯ '{keyword}' å®Œæˆï¼Œè·å– {len(notes)} æ¡æ•°æ®")
            except Exception as e:
                logger.error(f"å…³é”®è¯ '{keyword}' æœç´¢å¤±è´¥: {e}")
                results[keyword] = []
            
            # å…³é”®è¯ä¹‹é—´çš„å»¶è¿Ÿ
            if idx < len(keywords):
                await self.random_delay(3, 6)
        
        return results


# ä½¿ç”¨ç¤ºä¾‹
async def demo():
    """æ¼”ç¤ºå°çº¢ä¹¦é€‚é…å™¨ä½¿ç”¨"""
    async with XiaohongshuAdapter() as spider:
        # å•ä¸ªå…³é”®è¯æœç´¢
        notes = await spider.search_notes("ç‘œä¼½å«", max_pages=2)
        print(f"æ‰¾åˆ° {len(notes)} æ¡ç¬”è®°")
        
        # æ‰¹é‡æœç´¢
        results = await spider.batch_search(["ç‘œä¼½å«", "å¥èº«å™¨æ"], max_pages=2)
        for kw, notes in results.items():
            print(f"{kw}: {len(notes)} æ¡")


if __name__ == "__main__":
    import asyncio
    asyncio.run(demo())
